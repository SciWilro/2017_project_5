@article {Bycroft166298,
	author = {Bycroft, Clare and Freeman, Colin and Petkova, Desislava and Band, Gavin and Elliott, Lloyd T and Sharp, Kevin and Motyer, Allan and Vukcevic, Damjan and Delaneau, Olivier and O{\textquoteright}Connell, Jared and Cortes, Adrian and Welsh, Samantha and McVean, Gil and Leslie, Stephen and Donnelly, Peter and Marchini, Jonathan},
	title = {Genome-wide genetic data on ~500,000 UK Biobank participants},
	year = {2017},
	doi = {10.1101/166298},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The UK Biobank project is a large prospective cohort study of ~500,000 individuals from across the United Kingdom, aged between 40-69 at recruitment. A rich variety of phenotypic and health-related information is available on each participant, making the resource unprecedented in its size and scope. Here we describe the genome-wide genotype data (~805,000 markers) collected on all individuals in the cohort and its quality control procedures. Genotype data on this scale offers novel opportunities for assessing quality issues, although the wide range of ancestries of the individuals in the cohort also creates particular challenges. We also conducted a set of analyses that reveal properties of the genetic data (such as population structure and relatedness) that can be important for downstream analyses. In addition, we phased and imputed genotypes into the dataset, using computationally efficient methods combined with the Haplotype Reference Consortium (HRC) and UK10K haplotype resource. This increases the number of testable variants by over 100-fold to ~96 million variants. We also imputed classical allelic variation at 11 human leukocyte antigen (HLA) genes, and as a quality control check of this imputation, we replicate signals of known associations between HLA alleles and many common diseases. We describe tools that allow efficient genome-wide association studies (GWAS) of multiple traits and fast phenome-wide association studies (PheWAS), which work together with a new compressed file format that has been used to distribute the dataset. As a further check of the genotyped and imputed datasets, we performed a test-case genome-wide association scan on a well-studied human trait, standing height.},
	URL = {https://www.biorxiv.org/content/early/2017/07/20/166298},
	eprint = {https://www.biorxiv.org/content/early/2017/07/20/166298.full.pdf},
	journal = {bioRxiv}
}

@article{Kane2013,
abstract = {This paper presents two complementary statistical computing frameworks that address challenges in parallel processing and the analysis of massive data. First, the foreach package allows users of the R programming environment to define parallel loops that may be run sequentially on a single machine, in parallel on a symmetric multiprocessing (SMP) machine, or in cluster environments without platformspecific code. Second, the bigmemory package implements memoryand filemapped data structures that provide (a) access to arbitrarily large data while retaining a look and feel that is familiar to R users and (b) data structures that are shared across processor cores in order to support efficient parallel computing techniques. Although these packages may be used independently, this paper shows how they can be used in combination to address challenges that have effectively been beyond the reach of researchers who lack specialized software development skills or expensive hardware.},
author = {Kane, Michael J and Emerson, John W and Weston, Stephen},
doi = {10.18637/jss.v055.i14},
file = {:home/privef/Bureau/thesis-celiac/articles/Kane, Emerson, Weston - 2013 - Scalable Strategies for Computing with Massive Data.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
number = {14},
pages = {1--19},
title = {{Scalable Strategies for Computing with Massive Data}},
url = {http://www.jstatsoft.org/v55/i14/},
volume = {55},
year = {2013}
}

@article {Prive190926,
	author = {Privé, Florian and Aschard, Hugues and Blum, Michael G. B.},
	title = {Efficient management and analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr},
	year = {2017},
	doi = {10.1101/190926},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Genome-wide datasets produced for association studies have dramatically increased in size over the past few years, with modern datasets commonly including millions of variants measured in dozens of thousands of individuals. This increase in data size is a major challenge severely slowing down genomic analyses. Specialized software for every part of the analysis pipeline have been developed to handle large genomic data. However, combining all these software into a single data analysis pipeline might be technically difficult. Here we present two R packages, bigstatsr and bigsnpr, allowing for management and analysis of large scale genomic data to be performed within a single comprehensive framework. To address large data size, the packages use memory-mapping for accessing data matrices stored on disk instead of in RAM. To perform data pre-processing and data analysis, the packages integrate most of the tools that are commonly used, either through transparent system calls to existing software, or through updated or improved implementation of existing methods. In particular, the packages implement a fast derivation of Principal Component Analysis, functions to remove SNPs in Linkage Disequilibrium, and algorithms to learn Polygenic Risk Scores on millions of SNPs. We illustrate applications of the two R packages by analysing a case-control genomic dataset for the celiac disease, performing an association study and computing Polygenic Risk Scores. Finally, we demonstrate the scalability of the R packages by analyzing a simulated genome-wide dataset including 500,000 individuals and 1 million markers on a single desktop computer.},
	URL = {https://www.biorxiv.org/content/early/2017/09/19/190926},
	eprint = {https://www.biorxiv.org/content/early/2017/09/19/190926.full.pdf},
	journal = {bioRxiv}
}

@article{barrett2012ncbi,
  title={NCBI GEO: archive for functional genomics data sets—update},
  author={Barrett, Tanya and Wilhite, Stephen E and Ledoux, Pierre and Evangelista, Carlos and Kim, Irene F and Tomashevsky, Maxim and Marshall, Kimberly A and Phillippy, Katherine H and Sherman, Patti M and Holko, Michelle and others},
  journal={Nucleic acids research},
  volume={41},
  number={D1},
  pages={D991--D995},
  year={2012},
  publisher={Oxford University Press}
}

@article{anderson2010data,
  title={Data quality control in genetic case-control association studies},
  author={Anderson, Carl A and Pettersson, Fredrik H and Clarke, Geraldine M and Cardon, Lon R and Morris, Andrew P and Zondervan, Krina T},
  journal={Nature protocols},
  volume={5},
  number={9},
  pages={1564},
  year={2010},
  publisher={Europe PMC Funders}
}

@article{purcell2007plink,
  title={PLINK: a tool set for whole-genome association and population-based linkage analyses},
  author={Purcell, Shaun and Neale, Benjamin and Todd-Brown, Kathe and Thomas, Lori and Ferreira, Manuel AR and Bender, David and Maller, Julian and Sklar, Pamela and De Bakker, Paul IW and Daly, Mark J and others},
  journal={The American Journal of Human Genetics},
  volume={81},
  number={3},
  pages={559--575},
  year={2007},
  publisher={Elsevier}
}

@article{chang2015second,
  title={Second-generation PLINK: rising to the challenge of larger and richer datasets},
  author={Chang, Christopher C and Chow, Carson C and Tellier, Laurent CAM and Vattikuti, Shashaank and Purcell, Shaun M and Lee, James J},
  journal={Gigascience},
  volume={4},
  number={1},
  pages={7},
  year={2015},
  publisher={BioMed Central}
}


